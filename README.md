# Web Scraper and Knowledge Graph Generator

This project provides a Python script to scrape textual content from a given URL and then uses a locally run Hugging Face model (`google/flan-t5-base`) to convert this text into a structured knowledge graph in JSON format.

## Features

- Scrapes main textual content (headings, paragraphs, lists, links) from a webpage.
- Saves the raw scraped text to `content.txt`.
- Sends the text to a locally run Hugging Face model (`google/flan-t5-base`) to extract entities and relationships.
- Outputs a knowledge graph as a JSON array, with each object containing:
    - `head`: The source entity.
    - `head_type`: Category/type of the source entity.
    - `relation`: The relationship between entities.
    - `tail`: The target entity.
    - `tail_type`: Category/type of the target entity.
- Saves the generated knowledge graph to `knowledge_graph.json`.

## Setup

1.  **Clone the repository (if you haven't already):**
    ```bash
    git clone <repository_url>
    cd <repository_directory>
    ```

2.  **Create and activate a virtual environment (recommended):**
    ```bash
    python -m venv venv
    ```
    *   On macOS and Linux:
        ```bash
        source venv/bin/activate
        ```
    *   On Windows:
        ```bash
        venv\Scripts\activate
        ```

3.  **Install dependencies:**
    ```bash
    pip install -r requirements.txt
    ```
    (This will install `requests`, `beautifulsoup4`, `transformers`, `torch`, and `sentencepiece`.)

4.  **Model Download Note:**
    The first time you run the script, the Hugging Face model (`google/flan-t5-base`, which is approximately 1GB in size) will be downloaded and cached by the `transformers` library. This might take some time depending on your internet connection and can consume significant disk space. Subsequent runs will use the cached model. Ensure you have enough disk space and a stable internet connection for the initial download.

## Running the Script

To run the script, use `main.py` with the URL of the website you want to process:

```bash
python main.py <URL_OF_WEBSITE>
```

**Example:**

```bash
python main.py https://en.wikipedia.org/wiki/Artificial_intelligence
```
When running, you might see messages related to model loading and processing, which can take a few minutes depending on your hardware.

## Output

The script will produce the following files in the project's root directory:

-   **`content.txt`**: Contains the raw, unstructured text scraped from the provided URL.
-   **`knowledge_graph.json`**: Contains the structured knowledge graph in JSON format, generated by the Hugging Face model. This file is created if the model processing is successful.

The `knowledge_graph.json` file will contain a JSON array of objects, where each object follows this structure:

```json
[
  {
    "head": "Example Entity 1",
    "head_type": "Type A",
    "relation": "is_related_to",
    "tail": "Example Entity 2",
    "tail_type": "Type B"
  },
  // ... more objects
]
```
